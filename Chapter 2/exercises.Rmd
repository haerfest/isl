---
title: "Chapter 2 Exercises"
output: html_notebook
---

### Exercise 1

. | Flexible is | Justification
--|-------------|---------------
a | Better      | Extremely large sample set prevents overfitting the few predictors.
b | Worse       | Samples too few to reliably estimate relationships of many predictors. High risk of overfitting.
c | Better      | Non-linear relationship is hard to estimate with inflexible model.
d | Worse       | Lot of noise in the samples, do not want a flexible model to follow that.

### Exercise 2

. | Type           | Interest   | n   | p
--|----------------|------------|-----|---
a | Regression     | Inference  | 500 | 3
b | Classification | Prediction | 20  | 13
c | Regression     | Prediction | 52  | 3

### Exercise 3

```{r echo=FALSE}
x = 0:9
n = length(x)
y = seq(0, 15, length=n)

# irreducible error is a constant
bayes = rep(3.0, n)

# bias reduces as flexibility increases
bias = c(10, 5, 3, 1, 0.5, 0.25, 0.12, 0.06, 0.03, 0.015)

# variance increases with flexibility 
variance = rev(bias)

# test error follows bias and variance, stays above irreducible error
test = bayes + bias + variance

# train error will show overfitting as flexibility increases
train = append(test[1:4] - rep(0.5, 4), c(2, 1, 0.5, 0, 0, 0))

# plot them all together
plot(x, y, type="n", main="Errors", xaxt="n", yaxt="n", xlab="flexibility", ylab="error")
lines(x, bayes, col="black", lty=3)
lines(x, bias, col="green")
lines(x, variance, col="blue")
lines(x, test, col="red")
lines(x, train, col="gray")

# add legend
legend(x=mean(x), y=max(y), legend=c("Bayes", "Bias", "Variance", "Test", "Train"),
       col=c("black", "green", "blue", "red", "gray"), lty=c(3, 1, 1, 1, 1))
```
Curve    | Explanation
---------|-------------
Bayes    | Irreducible error due to predictors unaccounted for, noise. Constant regardless of model choice.
Bias     | Inflexible model has strong bias and potentially high error if it cannot model the data well. Reduces as flexibility increases.
Variance | Flexible model has high variance: small change in train set has strong impact on error. Reduces as flexibility reduces.
Test     | Follows sum of irreducible error (Bayes), bias and variance. Can never drop below irreducible error.
Train    | Initially follows same shape as test error, but an overly flexible model causes overfitting, reducing it to (close to) zero.

### Exercise 4

I will not.

### Exercise 5

Advantages:

* Can model very complex relationships between predictors and responses, more so than inflexible models.
* Does not suffer from strong bias, where it's not a good fit to the data.

Disadvantages:

* Hardly usable for inference, since exact relationship between predictors and responses is unclear.
* Risks overfitting, for instance when too little data is used.
* Suffers from strong variance when too flexible, where a small change in the training set used could yield a very different model.

Preferred when:

* An inflexible model is not accurate enough.

Less flexible model preferred when:

* Actually all the time, but it just doesn't always cut it so we have to resort to a more flexible approach.

### Exercise 6

* **Parametric**: you choose the model beforehand, and just need to fit its parameters.
* **Non-parametric**: you don't choose the model, but let it (and its parameters) be the result of training.

Advantages parametric approach:

* Simpler, only its parameters have to be estimated.
* Less risk of overfitting, since you typically choose a more biased model.

Disadvantages:

* May not give results as good as a non-parametric approach could, as that could end up with a more flexible model.

### Exercise 7

Obs.| X1 | X2 | X3 | Y     | Dist.
----|----|----|----|-------|-------
1   | 0  | 3  | 0  | Red   | 3
2   | 2  | 0  | 0  | Red   | 2
3   | 0  | 1  | 3  | Red   | 3.2
4   | 0  | 1  | 2  | Green | 2.2
5   | -1 | 0  | 1  | Green | 1.4
6   | 1  |1   | 1  | Red   | 1.7

* With K = 1 the prediction is Green, since the single closest neighbor is (-1, 0, 1) which has the color Green.
* With K = 3 the prediction is Red, since the three closest neighbors are (-1, 0, 1), (1, 1, 1), and (2, 0, 0), which have the colors Green, Red, and Red, respectively. The majority color is Red.
* The best value for K would be small, since it looks at only a few neighbors, which makes it follow the Bayes decision boundary better than a model with a large value of K.

### Exercise 8

```{r}
college = read.csv("College.csv")
rownames(college) = college[,1]
college = college[,-1]

summary(college)
```

```{r}
pairs(college[,1:10])
```

```{r}
plot(x=college$Private, y=college$Outstate, xlab="Private", ylab="Outstate", main="College")
```

```{r}
Elite = rep("No", nrow(college))
Elite[college$Top10perc > 50] = "Yes"
Elite = as.factor(Elite)
college = data.frame(college, Elite)
summary(college$Elite)
```

```{r}
plot(x=college$Elite, y=college$Outstate, xlab="Elite", ylab="Outstate", main="College")
```

```{r}
par(mfrow=c(2, 2))
hist(college$Room.Board, breaks=20)
hist(college$PhD, breaks=20)
hist(college$perc.alumni, breaks=20)
hist(college$Grad.Rate, breaks=20)
```

### Exercise 9

```{r}
Auto = na.omit(read.csv("Auto.csv", na.strings="?"))
```

Predictor    | Type         | `range`     | `mean`   | `sd` 
-------------|--------------|-------------|----------|----------
mpg          | Quantitative | 9.0 - 46.6  | 23.44592 | 7.805007
cylinders    | Qualitative  |             |          |
displacement | Quantitative | 68 - 455    | 194.412  | 104.644
horsepower   | Quantitative | 46 - 230    | 104.4694 | 38.49116
weight       | Quantitative | 1613 - 5140 | 2977.584 | 849.4026
acceleration | Quantitative | 8.0 - 24.8  | 15.54133 | 2.758864
year         | Quantitative | 70 - 82     | 75.97959 | 3.683737
origin       | Qualitative  |             |          |
name         | Qualitative  |             |          |

```{r}
Auto = Auto[-c(10:85),]
```

Predictor    | Type         | `range`     | `mean`   | `sd` 
-------------|--------------|-------------|----------|----------
mpg          | Quantitative | 11.0 - 46.6 | 24.40443 | 7.867283
displacement | Quantitative | 68 - 455    | 187.2405 | 99.67837
horsepower   | Quantitative | 46 - 230    | 100.7215 | 35.70885
weight       | Quantitative | 1649 - 4997 | 2935.972 | 811.3002
acceleration | Quantitative | 8.5 - 24.8  | 15.7269  | 2.693721
year         | Quantitative | 70 - 82     | 77.14557 | 3.106217

```{r}
pairs(~ Auto$mpg + Auto$year + Auto$displacement + Auto$horsepower + Auto$weight)
```

Yes:

* There appears to be a linear relationship between the `year` the car was built, and the `mpg` it achieves. The newer the car, the more miles per gallon it achieves.
* There appear to be logarithmic relationships between the engine `displacement`, `horsepower`, and the car's `weight`, with respect to the `mpg`. The bigger the engine, the more horsepower it produces, or the heavier the car, the smaller the miles per gallon the car manages. Of course those predictors are related: the heavier the car, the larger the engine it will have and the more horsepower it needs to get going.

### Exercise 10

```{r}
library(MASS)
attach(Boston)
```

506 rows and 14 columns. The rows are the samples, the columns the variables / features / predictors.

Especially `rm` and `lstat` seem to predict the `medv`:

* The more rooms the house has, the more expensive the house is.
* The smaller the percentage of the population that has a lower status, the more expensive the house is.

```{r}
pairs(~ medv + rm + lstat)
```

No linear relationship between `crim` and other predictors, but some interesting other relationships. For these predictors, per capita crime is generally low, except for:

* low values of `zn`: crime is higher in smaller areas?
* a particular middle value range of `indus`: crime is higher in areas that are partially residential and partially industrial, and lower in areas that are either fully residential or fully industrial.
* low values of `chas`: crime is lower close to the river.
* high values of `rad`: crime is lower when not close to highways.
* high values of `tax`: crime is higher for houses that are owned by rich people.
* high values of `ptratio`: crime is higher in areas with more pupils per teacher, or not enough teachers.

```{r}
pairs(crim ~ zn + indus + chas + rad + tax + ptratio)
```

```{r}
summary(crim)
```

75% of the records have a crime rate per capita of 3.67 or lower (3rd quantile). A plot shows there is a clear group of suburbs that have a higher crime rate. Most suburbs have a crime rate well below 10, the major group where crime thrives still lies below 40, and the outliers go all the way up to nearly 89.

```{r}
plot(crim)
```

There is also a group of suburbs that clearly have a much higher tax rate than the rest, except a very small group that tops themm all. Taxes range from $187 per $10,000 property value, to $711 per $10,000 property value. Most are well below $500.

```{r}
plot(tax)
```

There are also clusters of suburbs that have a substantially higher pupil-to-teacher ratio. The range goes from 12.6 pupils per teacher to almost twice that at 22.0 pupils per teacher. No substantial outliers:

```{r}
plot(ptratio)
```

```{r}
summary(as.factor(chas))
```

35 out of 506 suburbs bound the Charles river.

```{r}
median(ptratio)
```

The median pupil-to-teacher ratio is 19.05 pupils per teacher.

```{r}
Boston[medv==min(medv),]
```

There are two suburbs that qualify. Comparing predictors to what `summary` shows:

* crime per capita (`crim`) is exceptionally high at 38 and nearly 68.
* proportion of industry (`indus`) is at the third quantile level, so they are entering the top-25% of industrialized areas.
* same thing for nitrogen oxides concentrations (`nox`), possibly due to industry?
* they fall in the bottom-25% of having the smallest houses according to the number of rooms (`rm`).
* the houses are by far the oldest (`age`) of all suburbs.
* they are unusually close to employment centers (`dist`).
* they are unusually close to radial highways (`rad`).
* they enter the top-25% of highest tax rates (`tax`).
* they enter the top-25% of highest pupil-to-teacher ratio (`ptratio`).
* same story for one of the suburbs when it comes to propertion of black residents (`black`); the other suburb is close.
* unusually high percentage of the population has a lower status.

In summary: high crime rates; small, old houses; high tax rates; industrialized; polution; noway close to nature; poor education in terms of few teachers per pupil; low status residents.

```{r}
nrow(Boston[rm>7,])
```

64 suburbs average more than 7 rooms per dwelling.

```{r}
nrow(Boston[rm>8,])
```

13 suburbs average more than 8 rooms per dwelling.

Crime is mostly very low; very little industry; mostly not bounding the river; polution is about average; mostly old houses, mansions maybe; distance to employment centers slightly below average; mostly at great distance from highways; tax rate average; pupil-to-teacher ratio below average, so good education; above average proportion of black residents; low proportion of low-status residents; very expensive houses.

Suburbs for well-to-do people.