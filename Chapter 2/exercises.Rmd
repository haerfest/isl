---
title: "Chapter 2 Exercises"
output: html_notebook
---

### Exercise 1

. | Flexible is | Justification
--|-------------|---------------
a | Better      | Extremely large sample set prevents overfitting the few predictors.
b | Worse       | Samples too few to reliably estimate relationships of many predictors. High risk of overfitting.
c | Better      | Non-linear relationship is hard to estimate with inflexible model.
d | Worse       | Lot of noise in the samples, do not want a flexible model to follow that.

### Exercise 2

. | Type           | Interest   | n   | p
--|----------------|------------|-----|---
a | Regression     | Inference  | 500 | 3
b | Classification | Prediction | 20  | 13
c | Regression     | Prediction | 52  | 3

### Exercise 3

```{r echo=FALSE}
x = 0:9
n = length(x)
y = seq(0, 15, length=n)

# irreducible error is a constant
bayes = rep(3.0, n)

# bias reduces as flexibility increases
bias = c(10, 5, 3, 1, 0.5, 0.25, 0.12, 0.06, 0.03, 0.015)

# variance increases with flexibility 
variance = rev(bias)

# test error follows bias and variance, stays above irreducible error
test = bayes + bias + variance

# train error will show overfitting as flexibility increases
train = append(test[1:4] - rep(0.5, 4), c(2, 1, 0.5, 0, 0, 0))

# plot them all together
plot(x, y, type="n", main="Errors", xaxt="n", yaxt="n", xlab="flexibility", ylab="error")
lines(x, bayes, col="black", lty=3)
lines(x, bias, col="green")
lines(x, variance, col="blue")
lines(x, test, col="red")
lines(x, train, col="gray")

# add legend
legend(x=mean(x), y=max(y), legend=c("Bayes", "Bias", "Variance", "Test", "Train"),
       col=c("black", "green", "blue", "red", "gray"), lty=c(3, 1, 1, 1, 1))
```
Curve    | Explanation
---------|-------------
Bayes    | Irreducible error due to predictors unaccounted for, noise. Constant regardless of model choice.
Bias     | Inflexible model has strong bias and potentially high error if it cannot model the data well. Reduces as flexibility increases.
Variance | Flexible model has high variance: small change in train set has strong impact on error. Reduces as flexibility reduces.
Test     | Follows sum of irreducible error (Bayes), bias and variance. Can never drop below irreducible error.
Train    | Initially follows same shape as test error, but an overly flexible model causes overfitting, reducing it to (close to) zero.

### Exercise 4

I will not.

### Exercise 5

Advantages:

* Can model very complex relationships between predictors and responses, more so than inflexible models.
* Does not suffer from strong bias, where it's not a good fit to the data.

Disadvantages:

* Hardly usable for inference, since exact relationship between predictors and responses is unclear.
* Risks overfitting, for instance when too little data is used.
* Suffers from strong variance when too flexible, where a small change in the training set used could yield a very different model.

Preferred when:

* An inflexible model is not accurate enough.

Less flexible model preferred when:

* Actually all the time, but it just doesn't always cut it so we have to resort to a more flexible approach.

### Exercise 6

* **Parametric**: you choose the model beforehand, and just need to fit its parameters.
* **Non-parametric**: you don't choose the model, but let it (and its parameters) be the result of training.

Advantages parametric approach:

* Simpler, only its parameters have to be estimated.
* Less risk of overfitting, since you typically choose a more biased model.

Disadvantages:

* May not give results as good as a non-parametric approach could, as that could end up with a more flexible model.

### Exercise 7

Obs.| X1 | X2 | X3 | Y     | Dist.
----|----|----|----|-------|-------
1   | 0  | 3  | 0  | Red   | 3
2   | 2  | 0  | 0  | Red   | 2
3   | 0  | 1  | 3  | Red   | 3.2
4   | 0  | 1  | 2  | Green | 2.2
5   | -1 | 0  | 1  | Green | 1.4
6   | 1  |1   | 1  | Red   | 1.7

* With K = 1 the prediction is Green, since the single closest neighbor is (-1, 0, 1) which has the color Green.
* With K = 3 the prediction is Red, since the three closest neighbors are (-1, 0, 1), (1, 1, 1), and (2, 0, 0), which have the colors Green, Red, and Red, respectively. The majority color is Red.
* The best value for K would be small, since it looks at only a few neighbors, which makes it follow the Bayes decision boundary better than a model with a large value of K.

### Exercise 8

```{r}
college = read.csv("College.csv")
rownames(college) = college[,1]
college = college[,-1]

summary(college)
```

```{r}
pairs(college[,1:10])
```

```{r}
plot(x=college$Private, y=college$Outstate, xlab="Private", ylab="Outstate", main="College")
```

```{r}
Elite = rep("No", nrow(college))
Elite[college$Top10perc > 50] = "Yes"
Elite = as.factor(Elite)
college = data.frame(college, Elite)
summary(college$Elite)
```

```{r}
plot(x=college$Elite, y=college$Outstate, xlab="Elite", ylab="Outstate", main="College")
```

```{r}
par(mfrow=c(2, 2))
hist(college$Room.Board, breaks=20)
hist(college$PhD, breaks=20)
hist(college$perc.alumni, breaks=20)
hist(college$Grad.Rate, breaks=20)
```

### Exercise 9

```{r}
Auto = na.omit(read.csv("Auto.csv", na.strings="?"))
```

Predictor    | Type         | `range`     | `mean`   | `sd` 
-------------|--------------|-------------|----------|----------
mpg          | Quantitative | 9.0 - 46.6  | 23.44592 | 7.805007
cylinders    | Qualitative  |             |          |
displacement | Quantitative | 68 - 455    | 194.412  | 104.644
horsepower   | Quantitative | 46 - 230    | 104.4694 | 38.49116
weight       | Quantitative | 1613 - 5140 | 2977.584 | 849.4026
acceleration | Quantitative | 8.0 - 24.8  | 15.54133 | 2.758864
year         | Quantitative | 70 - 82     | 75.97959 | 3.683737
origin       | Qualitative  |             |          |
name         | Qualitative  |             |          |

```{r}
Auto = Auto[-c(10:85),]
```

Predictor    | Type         | `range`     | `mean`   | `sd` 
-------------|--------------|-------------|----------|----------
mpg          | Quantitative | 11.0 - 46.6 | 24.40443 | 7.867283
displacement | Quantitative | 68 - 455    | 187.2405 | 99.67837
horsepower   | Quantitative | 46 - 230    | 100.7215 | 35.70885
weight       | Quantitative | 1649 - 4997 | 2935.972 | 811.3002
acceleration | Quantitative | 8.5 - 24.8  | 15.7269  | 2.693721
year         | Quantitative | 70 - 82     | 77.14557 | 3.106217

```{r}
pairs(~ Auto$mpg + Auto$year + Auto$displacement + Auto$horsepower + Auto$weight)
```

Yes:

* There appears to be a linear relationship between the `year` the car was built, and the `mpg` it achieves. The newer the car, the more miles per gallon it achieves.
* There appear to be logarithmic relationships between the engine `displacement`, `horsepower`, and the car's `weight`, with respect to the `mpg`. The bigger the engine, the more horsepower it produces, or the heavier the car, the smaller the miles per gallon the car manages. Of course those predictors are related: the heavier the car, the larger the engine it will have and the more horsepower it needs to get going.
